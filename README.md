# Titulo: Asistente Virtual de Normativa Legal

## Descripci√≥n
Este proyecto implementa un asistente virtual de consulta de normativa legal en espa√±ol usando una arquitectura RAG (Retrieval-Augmented Generation).  
Se ingieren documentos PDF, se procesan (limpieza, chunking por secciones y tokens), se generan embeddings con **multilingual-e5-large-instruct**, se indexan en **Qdrant**, se emplea **Ollama** como gestor de LLM ([dolphin-mistral](https://ollama.com/library/dolphin-mistral)) y se exponen a trav√©s de una UI ligera con **Chainlit**

## Objetivo
Permitir a usuarios realizar preguntas en lenguaje natural sobre textos normativos (leyes, reglamentos, etc.) y obtener respuestas precisas basadas en el contenido real de los documentos.

## Descripci√≥n de los m√≥dulos empleados

1. **Ingesta de documentos**  
   - Descarga y almacenamiento de PDFs normativos en `data/raw/`.  
   - Extracci√≥n de texto con PyMuPDF (`extract_text.py`).

2. **Procesamiento y chunking**  
   - Limpieza de texto: eliminaci√≥n de cabeceras, espacios y saltos de l√≠nea innecesarios (`clean_text.py`).  
   - Divisi√≥n inteligente por secciones (t√≠tulos, art√≠culos) y/o tokens para garantizar que cada fragmento no supere el l√≠mite del modelo (`chunk_text_section.py`, `chunk_text.py`).

3. **Generaci√≥n de embeddings**  
   - Uso de `multilingual-e5-large-instruct` (SentenceTransformers) para convertir cada chunk en un vector de 1 024 dimensiones, normalizado en L2 (`embedder.py`).

4. **Indexado en Qdrant**  
   - Despliegue de Qdrant en Docker con almacenamiento persistente.  
   - Subida de vectores + metadatos (ID, texto) a la colecci√≥n `normativa` para b√∫squedas sem√°nticas de alta velocidad (`qdrant_uploader.py`).

5. **Pipeline RAG**  
   - Embedding de la consulta del usuario.  
   - Recuperaci√≥n de los Top-K chunks m√°s relevantes desde Qdrant.  
   - Construcci√≥n de prompt unificado:  
     ```
     Basado en la pregunta del usuario y el contexto dado, responde la pregunta:
     
     === CONTEXT ===
     <chunk1>
     ---
     <chunk2>
     ‚Ä¶
     
     User Query: <consulta>
     Answer:
     ```
   - Generaci√≥n de la respuesta con Ollama LLM (`dolphin-mistral`) a trav√©s de su API Python (`rag_pipeline.py`).

6. **Interfaz de usuario**  
   - Chat ligero construido con Chainlit (o Streamlit): permite conversaci√≥n en tiempo real, historial de preguntas y respuestas, y despliegue r√°pido en `http://localhost:8000` (`app.py`).

7. **Automatizaci√≥n y despliegue**  
   - Scripts de orquestaci√≥n (`build_index.py`, `run_app.sh`) para ejecutar todo el flujo end-to-end con un solo comando.  
   - Opcional: `docker-compose.yml` para levantar Qdrant y servicios auxiliares en producci√≥n.

## Arquitectura Global

```mermaid
---
config:
  look: neo
  layout: fixed
  theme: mc
---
flowchart LR
 subgraph Data["üìÇ data/"]
        A["raw PDFs"]
        B["extract_text.py"]
        C["clean_text.py"]
        D["chunk_text_section.py"]
  end
 subgraph Embeddings["üî¢ Generaci√≥n de Embeddings"]
        E["embedder.py"]
        F["*.json
        (id, text, vector)"]
  end
 subgraph VectorStore["üì¶ Qdrant"]
        G["qdrant_uploader.py"]
        H["Colecci√≥n normativa"]
  end
 subgraph RAG["ü§ñ Pipeline RAG"]
        I["User Query"]
        J["encode (user_query)"]
        K["Qdrant search
        (top-K vectors)"]
        L["chunks relevantes 
        (contextos)"]
        M["Build prompt
        (contextos + user_query)"]
        N["ollama.chat
        (dolphin-mistral)"]
        O["Respuesta generada"]
  end
 subgraph UI["üí¨ Interfaz"]
        P["app.py
        Chainlit"]
  end
    A --> B
    B --> C
    C --> D
    D --> E
    E --> F
    F --> G
    G --> H
    I --> J
    J --> K
    K --> H
    H --> L
    L --> M
    M --> N
    N --> O
    O --> P
    P --> I
```

![alt text](image.png)

## Gu√≠a de instalaci√≥n y puesta en marcha

1. **Clonar este repositorio**  

   ```bash
   git clone <tu-repo-url> rag-asistente-normativa
   cd rag-asistente-normativa
   ```

2. **Instalar Python > 3.12**

    Verifica:

    ```bash
    python --version
    ```
    2.1 Si no lo tienes, instala con pyenv (macOS/Linux):

    ```bash
    pyenv install 3.12.7
    pyenv local 3.12.7
    ```

    2.2 o desde python.org

3. **Crear y activar el virtualenv**
    ```bash
    uv sync
    source .venv/bin/activate
    python --version  # debe mostrar 3.12.x
    ```

4. **Instalar primero PyTorch (CPU)**
    ```bash
    uv pip install torch --index-url https://download.pytorch.org/whl/cpu
    ```

5. **Instalar el resto de dependencias**
    ```bash
    uv pip install -r pyproject.toml
    ```

    Tambi√©n se puede emplear uv sync y luego source .venv/bin/activate
6. **Configurar el IDE**

    Apuntar al int√©rprete .venv/bin/python.

7. **Docker & Qdrant**

    7.1 Instalar Docker Desktop y realizar login:

    ```bash
    docker login
    ```

    7.2 Descargar la imagen

    ```bash
    docker pull qdrant/qdrant
    ```

    7.3 Iniciar un contenedor persistente:

    ```bash
    docker run -d \
        --name qdrant_local \
        -p 6333:6333 \
        -v $(pwd)/qdrant_storage:/qdrant/storage \
        qdrant/qdrant
    ```

    7.4 Validar:

    ```bash
    docker ps
    ```

8. **Preprocesamiento y construcci√≥n del √≠ndice**

    ```bash
    python src/ingestion/extract_text.py
    python src/ingestion/clean_text.py
    python src/ingestion/chunk_text_section.py
    python src/embeddings/embedder.py
    python src/vector_store/qdrant_uploader.py
    ```

9. **Verifica el dashboard de Qdrant**

    Abre en tu navegador: http://localhost:6333/dashboard

10. **Instala y prepara Ollama**

    ```bash
    ollama ls
    ollama run dolphin-mistral
    ```

11. **Arranca la UI con Chainlit**

    ```bash
    ollama ls
    chainlit run src/ui/app.py
    ```
    
    Abre tu navegador en http://localhost:8000/

## Observaciones

Para chunks basados en tokens necesitar√°s tiktoken:

```bash
uv pip install tiktoken 
```

Puedes usar Streamlit en vez de Chainlit editando src/ui/app.py y lanzando:

```bash
streamlit run src/ui/app.py
```

## Referencias

- [RAG: Retrieval-Augmented Generation](https://medium.com/@tejpal.abhyuday/retrieval-augmented-generation-rag-from-basics-to-advanced-a2b068fd576c)
- [Chat Assistant using RAG](https://lightning.ai/lightning-ai/studios/document-chat-assistant-using-rag?section=featured)
- [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)
- [distiluse-base-multilingual-cased-v1](https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v1)
- [Qdrant Docker Hub](https://hub.docker.com/r/qdrant/qdrant)
- [Ollama Models](https://ollama.com/search)
- [Ollama Models: dolphin-mistral](https://ollama.com/library/dolphin-mistral)  
- [Chainlit](https://docs.chainlit.io/get-started/overview)
- [Anthropic: Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval)