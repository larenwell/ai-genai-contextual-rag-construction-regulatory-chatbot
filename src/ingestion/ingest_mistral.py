import os
import re
import base64
import json
from mistralai import Mistral
from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter
from typing import Dict, List, Optional, Any


class MistralExtractionController:
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.mistral_client = Mistral(api_key=api_key)
        
        # Inicializar text splitters
        self.markdown_splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=[
                ("#", "Header 1"),
                ("##", "Header 2"), 
                ("###", "Header 3"),
            ]
        )
        
        self.char_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", " ", ""]
        )
    
    def encode_pdf(self, pdf_path: str) -> str:
        try:
            with open(pdf_path, "rb") as pdf_file:
                return base64.b64encode(pdf_file.read()).decode('utf-8')
        except Exception as e:
            print(f"Error codificando PDF: {str(e)}")
            return None
    
    def extract_content_mistral_ocr(self, pdf_path: str) -> Optional[Dict[str, str]]:
        try:
            if not os.path.exists(pdf_path):
                print(f"Error: El archivo {pdf_path} no existe")
                return None
                
            print(f"Procesando archivo: {pdf_path}")
            
            # Codificar PDF a base64
            print("Codificando PDF a base64...")
            base64_pdf = self.encode_pdf(pdf_path)
            if not base64_pdf:
                return None
            
            # Llamar a la API OCR
            print("Procesando con Mistral OCR...")
            print(f"Tama√±o del PDF en base64: {len(base64_pdf)} caracteres")
            
            try:
                pdf_response = self.mistral_client.ocr.process(
                    model="mistral-ocr-latest",
                    document={
                        "type": "document_url",
                        "document_url": f"data:application/pdf;base64,{base64_pdf}"
                    },
                    include_image_base64=True
                )
                print("‚úÖ Respuesta de Mistral OCR recibida")
            except Exception as api_error:
                print(f"‚ùå Error en la llamada a Mistral OCR: {str(api_error)}")
                print(f"üìã Tipo de error: {type(api_error).__name__}")
                raise
            
            # Convertir respuesta a formato JSON
            response_dict = json.loads(pdf_response.model_dump_json())
            print(f"üìã Estructura de respuesta: {list(response_dict.keys())}")
            
            # Extraer el contenido markdown
            # La estructura de respuesta de Mistral OCR tiene el contenido en pages[0]['markdown']
            pages = response_dict.get("pages", [])
            if pages and len(pages) > 0:
                markdown_content = pages[0].get("markdown", "")
                print(f"‚úÖ Contenido extra√≠do de pages[0]['markdown']: {len(markdown_content)} caracteres")
            else:
                # Fallback al campo 'content' directo
                markdown_content = response_dict.get("content", "")
                print(f"‚ö†Ô∏è  Usando fallback 'content': {len(markdown_content)} caracteres")
            
            if not markdown_content:
                print("‚ùå Error: No se pudo extraer contenido del PDF")
                print(f"üìã Claves disponibles en response_dict: {list(response_dict.keys())}")
                print(f"üìã N√∫mero de p√°ginas: {len(pages)}")
                if pages:
                    print(f"üìã Claves de pages[0]: {list(pages[0].keys())}")
                return None
            
            print(f"Contenido extra√≠do exitosamente: {len(markdown_content)} caracteres")
            
            return {
                "markdown_content": markdown_content,
                "response_data": response_dict,
                "original_filename": os.path.basename(pdf_path)
            }
                
        except Exception as e:
            print(f"Error al extraer contenido del PDF: {str(e)}")
            return None
    
    # Idea principal de todo el texto
    def generate_document_summary(self, markdown_content: str) -> str:
        try:
            # Limitar el contenido si es muy largo
            content_preview = markdown_content[:8000] if len(markdown_content) > 8000 else markdown_content
            
            summary_response = self.mistral_client.chat.complete(
                model="mistral-small-latest",
                messages=[
                    {
                        "role": "system",
                        "content": """Eres un experto en an√°lisis de documentos t√©cnicos. Tu tarea es generar un resumen conciso pero completo del documento que capture:
                        
                        1. **Tema principal y objetivos**: ¬øDe qu√© trata el documento?
                        2. **Conceptos clave**: Terminolog√≠a y conceptos importantes
                        3. **Estructura del contenido**: Organizaci√≥n y secciones principales
                        4. **Elementos t√©cnicos**: Tablas, f√≥rmulas, diagramas relevantes
                        5. **Contexto de aplicaci√≥n**: √Åmbito de uso del documento
                        
                        El resumen debe ser √∫til para contextualizar fragmentos espec√≠ficos del documento en un sistema RAG."""
                    },
                    {
                        "role": "user", 
                        "content": f"Analiza y resume este documento t√©cnico:\n\n{content_preview}"
                    }
                ]
            )
            
            return summary_response.choices[0].message.content
            
        except Exception as e:
            print(f"Error generando resumen: {str(e)}")
            return "No se pudo generar resumen del documento."
    
    # Chunking basado en headers
    def intelligent_chunking(self, markdown_content: str) -> List[Dict[str, Any]]:
        chunks = []
        
        try:
            # Intentar chunking por headers primero
            header_chunks = self.markdown_splitter.split_text(markdown_content)
            
            if len(header_chunks) > 1:
                print(f"Usando chunking por headers: {len(header_chunks)} chunks")
                base_chunks = header_chunks
            else:
                print("No se encontraron headers, usando chunking por caracteres")
                base_chunks = [{"page_content": markdown_content, "metadata": {}}]
                
        except Exception as e:
            print(f"Error en chunking por headers: {e}")
            base_chunks = [{"page_content": markdown_content, "metadata": {}}]
        
        # Procesar cada chunk base
        for i, chunk in enumerate(base_chunks):
            if hasattr(chunk, 'page_content'):
                content = chunk.page_content
                metadata = getattr(chunk, 'metadata', {})
            elif isinstance(chunk, dict):
                content = chunk.get("page_content", chunk)
                metadata = chunk.get("metadata", {})
            else:
                content = chunk
                metadata = {}
            
            # Si el chunk es muy grande, subdivirlo
            if len(content) > 1200:
                sub_chunks = self.char_splitter.split_text(content)
                for j, sub_chunk in enumerate(sub_chunks):
                    chunks.append({
                        "content": sub_chunk,
                        "metadata": {
                            **metadata,
                            "chunk_id": f"{i}_{j}",
                            "chunk_type": "text_subdivision",
                            "parent_chunk": i
                        }
                    })
            else:
                chunks.append({
                    "content": content,
                    "metadata": {
                        **metadata,
                        "chunk_id": str(i),
                        "chunk_type": "structured_section" if metadata else "full_text"
                    }
                })
        
        return chunks
    
    # Extraer elementos visuales de un chunk
    def extract_visual_elements(self, chunk_content: str) -> Dict[str, Any]:
        visual_info = {
            "has_images": False,
            "has_tables": False,
            "has_formulas": False,
            "has_lists": False,
            "image_descriptions": [],
            "table_count": 0,
            "formula_count": 0,
            "list_count": 0
        }
        
        # Detectar im√°genes
        image_patterns = [
            r'!\[.*?\]\(.*?\)',  # Markdown images
            r'<img[^>]*>',       # HTML images
            r'imagen\s*\d+',     # Referencias a im√°genes
            r'figura\s*\d+',     # Referencias a figuras
        ]
        
        for pattern in image_patterns:
            matches = re.findall(pattern, chunk_content, re.IGNORECASE)
            if matches:
                visual_info["has_images"] = True
                visual_info["image_descriptions"].extend(matches)
        
        # Detectar tablas
        table_patterns = [
            r'\|.*?\|',          # Markdown tables
            r'<table[^>]*>',     # HTML tables
            r'tabla\s*\d+',      # Referencias a tablas
        ]
        
        for pattern in table_patterns:
            matches = re.findall(pattern, chunk_content, re.IGNORECASE | re.MULTILINE)
            if matches:
                visual_info["has_tables"] = True
                visual_info["table_count"] += len(matches)
        
        # Detectar f√≥rmulas
        formula_patterns = [
            r'\$.*?\$',          # LaTeX inline
            r'\$\$.*?\$\$',      # LaTeX block
            r'\\\(.*?\\\)',      # LaTeX parentheses
            r'\\\[.*?\\\]',      # LaTeX brackets
        ]
        
        for pattern in formula_patterns:
            matches = re.findall(pattern, chunk_content, re.DOTALL)
            if matches:
                visual_info["has_formulas"] = True
                visual_info["formula_count"] += len(matches)
        
        # Detectar listas
        list_patterns = [
            r'^\s*[-*+]\s+',     # Unordered lists
            r'^\s*\d+\.\s+',     # Ordered lists
        ]
        
        for pattern in list_patterns:
            matches = re.findall(pattern, chunk_content, re.MULTILINE)
            if matches:
                visual_info["has_lists"] = True
                visual_info["list_count"] += len(matches)
        
        return visual_info
    
    # Contextualizar un chunk especifico
    def contextualize_chunk(self, chunk: Dict[str, Any], document_summary: str, 
                          book_title: str, page_num: int) -> Dict[str, Any]:
        chunk_content = chunk["content"]
        visual_info = self.extract_visual_elements(chunk_content)
        
        # Crear contexto visual
        visual_context = []
        if visual_info["has_images"]:
            visual_context.append(f"Contiene {len(visual_info['image_descriptions'])} im√°genes/figuras")
        if visual_info["has_tables"]:
            visual_context.append(f"Contiene {visual_info['table_count']} tablas")
        if visual_info["has_formulas"]:
            visual_context.append(f"Contiene {visual_info['formula_count']} f√≥rmulas")
        if visual_info["has_lists"]:
            visual_context.append(f"Contiene {visual_info['list_count']} listas")
        
        visual_summary = "; ".join(visual_context) if visual_context else "Solo texto"
        
        # Crear prompt contextual
        context_prompt = f"""
DOCUMENTO: {book_title}
P√ÅGINA: {page_num}

RESUMEN DEL DOCUMENTO:
{document_summary}

CONTENIDO DEL FRAGMENTO:
{chunk_content}

ELEMENTOS VISUALES: {visual_summary}

INSTRUCCIONES:
Crea un resumen contextualizado de este fragmento que:
1. Explique el contenido en relaci√≥n al documento completo
2. Preserve la informaci√≥n t√©cnica clave
3. Describa brevemente los elementos visuales si existen
4. Mantenga terminolog√≠a espec√≠fica para b√∫squeda
5. Sea conciso pero informativo (m√°ximo 300 palabras)

IMPORTANTE: Mant√©n el lenguaje t√©cnico original y conceptos espec√≠ficos.
"""
        
        try:
            context_response = self.mistral_client.chat.complete(
                model="mistral-small-latest",  # Corregido el modelo
                messages=[
                    {
                        "role": "system",
                        "content": "Eres un experto en crear res√∫menes contextualizados para sistemas RAG. Preserva la informaci√≥n t√©cnica y estructura del contenido original."
                    },
                    {
                        "role": "user",
                        "content": context_prompt
                    }
                ]
            )
            
            contextualized_content = context_response.choices[0].message.content
            
        except Exception as e:
            print(f"Error contextualizando chunk {chunk.get('metadata', {}).get('chunk_id', 'unknown')}: {str(e)}")
            contextualized_content = chunk_content
        
        # Metadata enriquecida
        enhanced_metadata = {
            **chunk.get("metadata", {}),
            "book_title": book_title,
            "page_number": page_num,
            "original_content": chunk_content,
            "contextualized_content": contextualized_content,
            "visual_summary": visual_summary,
            **visual_info
        }
        
        return {
            "content": contextualized_content,
            "metadata": enhanced_metadata
        }
    
    # Funcion principal
    def process_document(self, pdf_path: str, book_title: str = None) -> Optional[List[Dict[str, Any]]]:
        if not book_title:
            book_title = os.path.basename(pdf_path)
        
        print(f"=== Procesando documento: {book_title} ===")
        
        # 1. Extracci√≥n OCR
        print("1. Extrayendo contenido con Mistral OCR...")
        extraction_result = self.extract_content_mistral_ocr(pdf_path)
        if not extraction_result:
            return None
        
        # 2. Generar resumen
        print("2. Generando resumen del documento...")
        document_summary = self.generate_document_summary(extraction_result["markdown_content"])
        
        # 3. Chunking inteligente
        print("3. Realizando chunking inteligente...")
        chunks = self.intelligent_chunking(extraction_result["markdown_content"])
        print(f"   Generados {len(chunks)} chunks")
        
        # 4. Contextualizar chunks
        print("4. Contextualizando chunks...")
        enhanced_chunks = []
        for i, chunk in enumerate(chunks):
            print(f"   Procesando chunk {i+1}/{len(chunks)}")
            enhanced_chunk = self.contextualize_chunk(
                chunk, document_summary, book_title, 1
            )
            enhanced_chunks.append(enhanced_chunk)
        
        print(f"Proceso completado: {len(enhanced_chunks)} chunks contextualizados")
        return enhanced_chunks